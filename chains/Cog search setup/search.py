import os
import json 
import requests
import openai
from dotenv import load_dotenv
from langchain.embeddings import OpenAIEmbeddings

load_dotenv()

COGNITIVE_SEARCH_INDEX = os.getenv("COGNITIVE_SEARCH_INDEX")
COGNITIVE_SEARCH_HOSTNAME = os.getenv("COGNITIVE_SEARCH_HOSTNAME")
COGNITIVE_SEARCH_API_KEY = os.getenv("COGNITIVE_SEARCH_API_KEY")
COGNITIVE_SEARCH_API_VERSION = os.getenv("COGNITIVE_SEARCH_API_VERSION")

openai.api_type = "azure"
openai.api_version = "2023-05-15"
openai.api_base = os.getenv('OPENAI_API_BASE')
openai.api_key = os.getenv("OPENAI_API_KEY")


embeddings = OpenAIEmbeddings(model="text-embedding-ada-002", chunk_size=1)

headers = {
    "Content-Type": "application/json",
    "api-key": COGNITIVE_SEARCH_API_KEY
}

search_query = "U2 generator site"
search_vector = embeddings.embed_query(search_query)

# This would need to be generated by the output of the first step of the LLM
query = {
    "search": search_query,
    "vector": {
        "value": search_vector,
        "fields": "issue_statement_vector",
        "k": 10
    },
    "filter": "unit_name eq 'ABC Company 2'",
    "select": "pcm, unit_name, issue_statement",
    "top": 10
}
 
# print(f"Search query is: {json.dumps(query, indent=2)}")

url = f"{COGNITIVE_SEARCH_HOSTNAME}/indexes/{COGNITIVE_SEARCH_INDEX}/docs/search?api-version={COGNITIVE_SEARCH_API_VERSION}"
response = requests.post(url, headers=headers, json=query)
print(f"Ran search - Status {response.status_code}, message: {response.text}")

print(f"Results: {response.json()}")